# Documentation: How Embeddings Detect Plagiarism

This document explains the underlying principles of using text embeddings for plagiarism detection, as implemented in this application.

## What are Text Embeddings?

Text embeddings are numerical representations of text (words, sentences, paragraphs, or even entire documents) in a high-dimensional vector space. The key idea is that texts with similar meanings will have similar vector representations, meaning they will be close to each other in this vector space. These embeddings are typically generated by pre-trained deep learning models (like BERT, Sentence-Transformers, or OpenAI's GPT models).

## How Semantic Similarity is Measured

Once texts are converted into embeddings, their semantic similarity can be measured using mathematical techniques. The most common method is **Cosine Similarity**.

### Cosine Similarity

Cosine similarity measures the cosine of the angle between two non-zero vectors. It determines whether two vectors are pointing in roughly the same direction. A cosine similarity of:
- **1** indicates that the vectors are identical in direction (perfectly similar).
- **0** indicates that the vectors are orthogonal (no similarity).
- **-1** indicates that the vectors are diametrically opposed (completely dissimilar).

In the context of text embeddings, a higher cosine similarity score between two text embeddings implies a higher degree of semantic similarity between the original texts.

## Plagiarism Detection with Embeddings

The process of detecting plagiarism using embeddings involves the following steps:

1.  **Text Preprocessing**: Input texts are cleaned and prepared for embedding generation. This might involve tokenization, lowercasing, removing stop words, etc., although many modern embedding models handle much of this internally.

2.  **Embedding Generation**: Each input text is passed through an embedding model (e.g., `SentenceTransformer` or OpenAI's embedding API) to obtain its numerical vector representation.

3.  **Pairwise Similarity Calculation**: For every unique pair of texts, the cosine similarity between their respective embeddings is calculated. This results in a similarity matrix where each cell `(i, j)` represents the similarity between Text `i` and Text `j`.

4.  **Clone Detection**: A predefined similarity threshold (e.g., 80% or 0.8) is used to identify potential plagiarism. If the cosine similarity between two texts exceeds this threshold, they are flagged as potential clones. This indicates that their semantic content is highly similar, suggesting one might be derived from the other.

5.  **Visualization and Reporting**: The similarity matrix is displayed, often with color-coding or highlighting to visually represent similarity levels. Potential clone pairs are explicitly listed, providing a clear overview of suspected plagiarism.

## Advantages of Embedding-Based Plagiarism Detection

-   **Semantic Understanding**: Unlike traditional plagiarism detectors that rely heavily on exact string matching or n-gram overlaps, embedding-based methods understand the meaning (semantics) of the text. This allows them to detect plagiarism even when the text has been rephrased, paraphrased, or translated.
-   **Robustness to Minor Changes**: Small changes in wording, sentence structure, or word order do not significantly alter the embedding, making the detection more robust.
-   **Cross-Lingual Potential**: Some advanced embedding models can generate embeddings for texts in different languages that are semantically similar, opening possibilities for cross-lingual plagiarism detection.

## Limitations and Considerations

-   **Model Dependence**: The accuracy of detection heavily relies on the quality and training of the embedding model. Different models may yield different similarity scores.
-   **Threshold Sensitivity**: The chosen similarity threshold is crucial. A too-high threshold might miss subtle plagiarism, while a too-low threshold might flag too many false positives.
-   **Contextual Nuances**: While good at semantics, embeddings might sometimes miss very subtle contextual nuances or intent, especially in highly specialized domains.
-   **Computational Cost**: Generating embeddings for very large documents or a large number of documents can be computationally intensive.

By leveraging the power of text embeddings, this application provides a sophisticated tool for identifying semantic similarities between texts, offering a more advanced approach to plagiarism detection.